Practical 1:
	Let us consider the following dataset from normal distribution N(Œº, 1). Let us also consider Œº ~ ùìù(œÑ, 2) is the prior distribution. Obtain the posterior distribution for the parameter Œº and plot the prior and posterior distribution in the same graph for œÑ = 10, 15 and 20. Also, obtain the posterior mean for each value of the hyperparameter œÑ.
9.48, 17.15, 11.68, 10.00, 11.88, 12.13, 14.93, 9.73, 7.61, 7.21, 10.28, 7.03, 11.01, 12.36, 13.28, 6.58, 11.07, 12.56, 8.18, 9.17

# R code
data <- c(9.48, 17.15, 11.68, 10.00, 11.88, 12.13, 14.93, 9.73, 7.61, 7.21,
          10.28, 7.03, 11.01, 12.36, 13.28, 6.58, 11.07, 12.56, 8.18, 9.17)

# Constants
n <- length(data)
x_bar <- mean(data)
sigma2 <- 1        # Known variance of data
sigma02 <- 2       # Prior variance

# œÑ values to test
tau_values <- c(10, 15, 20)

# Range for plotting
mu_vals <- seq(5, 25, length.out = 1000)

# Plot prior and posterior for each œÑ
par(mfrow = c(1, 3))
posterior_means <- c()

for (tau in tau_values) {
  # Posterior variance
  sigma_n2 <- 1 / (n / sigma2 + 1 / sigma02)
  
  # Posterior mean
  mu_n <- sigma_n2 * ((n * x_bar / sigma2) + (tau / sigma02))
  
  # Store posterior mean
  posterior_means <- c(posterior_means, mu_n)
  
  # Prior and posterior densities
  prior_density <- dnorm(mu_vals, mean = tau, sd = sqrt(sigma02))
  posterior_density <- dnorm(mu_vals, mean = mu_n, sd = sqrt(sigma_n2))
  
  # Plot
  plot(mu_vals, prior_density, type = "l", col = "blue", lwd = 2,
       ylab = "Density", xlab = expression(mu), main = bquote(tau == .(tau)))
  lines(mu_vals, posterior_density, col = "red", lwd = 2)
  legend("topright", legend = c("Prior", "Posterior"),
         col = c("blue", "red"), lwd = 2)
}

# Output posterior means
posterior_table <- data.frame(tau = tau_values, PosteriorMean = posterior_means)
print(posterior_table)

Practical 2:
# Let us consider the following dataset follows an exponential distribution with scale parameter Œ∏. Let us consider the prior 1/Œ∏. Obtain posterior distribution, Bayes estimator, and 0.95 HPD interval for the parameter Œ∏.


# Given Data
x <- c(3.29, 7.53, 0.48, 2.03, 0.36, 0.07, 4.49, 1.05, 9.15,
       3.67, 2.22, 2.16, 4.06, 11.62, 8.26, 1.96, 9.13, 1.78, 3.81, 17.02)

n <- length(x)
S <- sum(x)

# Create grid of theta values
theta_grid <- seq(0.01, 100, length.out = 10000)

# Posterior density (up to normalization): Inverse Gamma(n, S)
posterior_unnormalized <- theta_grid^(-n - 1) * exp(-S / theta_grid)

# Normalize the posterior
posterior_density <- posterior_unnormalized / sum(posterior_unnormalized)

# Bayes estimator (posterior mean)
posterior_mean <- sum(theta_grid * posterior_density)
cat("Bayes estimator (posterior mean):", posterior_mean, "\n")

# Compute HPD interval
sorted_indices <- order(posterior_density, decreasing = TRUE)
sorted_theta <- theta_grid[sorted_indices]
sorted_density <- posterior_density[sorted_indices]

# Cumulative sum to find 95% highest mass
cum_probs <- cumsum(sorted_density)
cutoff_index <- which(cum_probs >= 0.95)[1]
hpd_theta_values <- sorted_theta[1:cutoff_index]

# HPD interval = range of selected theta values
hpd_interval <- range(hpd_theta_values)
cat("HPD Interval: [", hpd_interval[1], ",", hpd_interval[2], "]\n")

# Plot posterior density
plot(theta_grid, posterior_density, type = "l", col = "blue", lwd = 2,
     main = "Posterior Distribution with HPD Interval",
     xlab = expression(theta), ylab = "Posterior Density")

# Add vertical lines for HPD bounds
abline(v = hpd_interval, col = "red", lwd = 2, lty = 2)

# Add posterior mean
abline(v = posterior_mean, col = "darkgreen", lwd = 2, lty = 3)

# Add legend
legend("topright", legend = c("Posterior", "HPD Interval", "Posterior Mean"),
       col = c("blue", "red", "darkgreen"), lty = c(1, 2, 3), lwd = 2)

# Practical 3:
	Consider the situation where a child is given an intelligence test. Assume that the test result X is N(Œ∏,100), where Œ∏ is the true I.Q. (intelligence) level of the child, as measured by the test. In other words, if the child were to take a large number of independent similar tests, his average score would be about Œ∏. Assume also that the population, as a whole, Œ∏ is distributed according to a N(100,225) distribution. Thus, if a child scores 115 on the test, then:
a. Obtain the distribution of his true IQ.
b. Obtain Bayes' decision under the following set up of actions and loss functions:
# Prior: Œ∏ ~ N(100, 225)
mu0 <- 100
var0 <- 225

# Likelihood: X ~ N(Œ∏, 100), observed X = 115
x <- 115
var_x <- 100

# Posterior mean and variance
post_var <- 1 / (1 / var0 + 1 / var_x)
post_mean <- post_var * (mu0 / var0 + x / var_x)

cat("Posterior: N(", round(post_mean, 2), ",", round(post_var, 2), ")\n")


# Posterior distribution of theta ~ N(post_mean, post_var)
theta_vals <- seq(50, 150, length.out = 1000)
posterior_density <- dnorm(theta_vals, mean = post_mean, sd = sqrt(post_var))
# Loss functions:
L1 <- ifelse(theta_vals < 90, 0,
             ifelse(theta_vals <= 110, theta_vals - 90, 2 * (theta_vals - 90)))

L2 <- ifelse(theta_vals < 90, 90 - theta_vals,
             ifelse(theta_vals <= 110, 0, theta_vals - 110))

L3 <- ifelse(theta_vals <= 110, 110 - theta_vals, 0)

# Expected loss for each action
expected_L1 <- sum(L1 * posterior_density) * diff(theta_vals[1:2])
expected_L2 <- sum(L2 * posterior_density) * diff(theta_vals[1:2])
expected_L3 <- sum(L3 * posterior_density) * diff(theta_vals[1:2])

losses <- c(expected_L1, expected_L2, expected_L3)
names(losses) <- c("Below average", "Average", "Above average")

print(round(losses, 4))

# Bayes action: action with minimum expected loss
best_action <- names(losses)[which.min(losses)]
cat("Bayes Action: IQ is", best_action, "\n")


Practical 4:
		The electroweak theory predicted the existence of a new particle, the W particle, of a mass m of 82.4 ¬± 1.1 GeV. Experimental results showed that such a particle existed and had a mass of 82.1 ¬± 1.7 GeV. If we take the mass to have a normal prior and likelihood and assume that the values after the ¬± signs represent known standard deviations, and if we are prepared to take both the theory and the experiment into account, then:
A. Obtain the posterior distribution of mass.
B. Suppose that, for some reason, it was important to know whether or not this mass was less than 83.0 GeV. Obtain the Bayes factor for testing the above hypothesis.

# Prior parameters
mu0 <- 82.4
sigma0 <- 1.1

# Likelihood parameters
x <- 82.1
sigma <- 1.7

# Posterior variance and mean
post_var <- 1 / (1 / sigma0^2 + 1 / sigma^2)
post_mean <- post_var * (mu0 / sigma0^2 + x / sigma^2)

cat("Posterior: N(", round(post_mean, 4), ",", round(sqrt(post_var), 4), ")\n")

# Bayes factor for H0: m < 83 vs H1: m ‚â• 83
p_less_83 <- pnorm(83, mean = post_mean, sd = sqrt(post_var))
p_ge_83 <- 1 - p_less_83

BF <- p_less_83 / p_ge_83

cat("Bayes Factor (H0: m < 83 vs H1: m ‚â• 83):", round(BF, 4), "\n")
