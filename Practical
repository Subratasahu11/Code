#Time Series 
library(datasets)
data=Nile
plot(data)
require(graphics)
data1<-ts(data,start=c(1871,1), end=c(1970,12), frequency=12)
decomp<-decompose(data1) 
plot(decomp)
Autocorr<-acf(data1,lag.max = 3,plot=FALSE)
Autocorr
Partial_Autocorr<-acf(data1,lag.max = 3,type=c("partial"),plot=FALSE)
Partial_Autocorr
acf(data1, lag.max = 3, main = "Autocorrelation Function")
pacf(data1,lag.max = 3, main = "Partial Autocorrelation Function")







#Machine Learning 1
data <- c(5.65746599, 5.38283914, 2.79892121, 2.85423660, 2.95252721, 5.42626667,
          7.66239113, -0.18001073, 0.65083500, 2.40276530, -0.09929884, 6.32619215,
          5.03650752, 2.07470777, 1.78019174, 6.12891558, 4.05352439, 2.02686971,
          3.50834853, -2.76449768, 4.98428763, 3.01292677, 2.82448038, 3.98110437,
          5.09371862, 5.97961648, 4.56968496, -0.48814532, 5.08736697, 2.41757609)
kde <- density(data)
kde
naive_density <- function(x, data, h) {
  n <- length(data)
  return(sum((abs(x - data) <= h) / (2 * h)) / n)
}
h <- 1.0 
x_range <- seq(min(data) - 1, max(data) + 1, length.out = 100)
naive_estimates <- sapply(x_range, naive_density, data = data, h = h)
plot(kde, main = "Kernel Density Estimate vs Naïve Density Estimator",
     col = "blue", lwd = 2, ylim = c(0, max(kde$y, naive_estimates)), xlab = "X", ylab = "Density")
lines(x_range, naive_estimates, col = "red", lwd = 2)
legend("topright", legend = c("KDE", "Naïve Density"), col = c("blue", "red"), lty = 1, lwd = 2)








#Machine Learning 2
data("birthwt", package = "MASS")
birthwt <- birthwt
birthwt$low <- as.numeric(birthwt$low) 
train_indices <- sample(1:nrow(birthwt), size = 0.7 * nrow(birthwt)) 
train_data <- birthwt[train_indices, ]
test_data <- birthwt[-train_indices, ]
logistic <- function(x) {
  1 / (1 + exp(-x))
}
fit_logistic <- function(X, y, max_iter = 100, tol = 1e-6) {
  n <- nrow(X)
  p <- ncol(X)
  beta <- matrix(0, nrow = p, ncol = 1)
  epsilon <- 1e-8 
  for (i in 1:max_iter) {
    eta <- X %*% beta 
    mu <- logistic(eta) 
    mu <- pmin(pmax(mu, epsilon), 1 - epsilon)
    W <- diag(as.vector(mu * (1 - mu)), n, n) 
    z <- eta + (y - mu) / (mu * (1 - mu))
    Hessian <- t(X) %*% W %*% X
    gradient <- t(X) %*% (y - mu)
    beta_new <- beta + solve(Hessian + diag(epsilon, p)) %*% gradient 
    
    if (any(is.na(beta_new))) stop("Divergence detected in Newton-Raphson method.")
    if (max(abs(beta_new - beta)) < tol) break
    
    beta <- beta_new
  }
  return(beta)
}
X_train <- cbind(1, as.matrix(train_data[, -1])) 
y_train <- train_data$low
beta <- fit_logistic(X_train, y_train)
logistic_model_eq <- paste("log(odds) =", paste(round(beta, 4), c("(Intercept)", colnames(train_data)[-
                                                                                                        1]), collapse = " + "))
cat("Logistic Model Equation:\n", logistic_model_eq, "\n\n")

X_test <- cbind(1, as.matrix(test_data[, -1]))
y_test <- test_data$low
log_odds <- X_test %*% beta
probs <- logistic(log_odds)
predictions <- ifelse(probs > 0.5, 1, 0)

confusion_matrix <- table(Predicted = predictions, Actual = y_test)
cat("Confusion Matrix:\n")
print(confusion_matrix)

true_positive <- confusion_matrix["1", "1"]
true_negative <- confusion_matrix["0", "0"]
false_positive <- confusion_matrix["1", "0"]
false_negative <- confusion_matrix["0", "1"]
sensitivity <- true_positive / (true_positive + false_negative)
specificity <- true_negative / (true_negative + false_positive)
positive_predictive_value <- true_positive / (true_positive + false_positive)
negative_predictive_value <- true_negative / (true_negative + false_negative)

cat("\nModel Performance Metrics:\n")
cat("Sensitivity (True Positive Rate):", round(sensitivity, 4), "\n")
cat("Specificity (True Negative Rate):", round(specificity, 4), "\n")
cat("Positive Predictive Value (PPV):", round(positive_predictive_value, 4), "\n")
cat("Negative Predictive Value (NPV):", round(negative_predictive_value, 4), "\n")







#Machine Learning 3
shape <- 2
scale <- 5
data <- rgamma(100, shape = shape, scale = scale)
bandwidths <- c(0.1, 0.2, 0.5)
plot(NULL, xlim = range(data), ylim = c(0, 0.3), 
     xlab = "Value", ylab = "Density", 
     main = "Kernel Density Estimates with Gaussian Kernel")
colors <- c("red", "blue", "green")
for (i in 1:length(bandwidths)) {
  bw <- bandwidths[i]
  density_estimate <- density(data, kernel = "gaussian", bw = bw)
  lines(density_estimate, col = colors[i], lwd = 2)
}
legend("topright", legend = paste("Bandwidth =", bandwidths), 
       col = colors, lwd = 2, cex = 1)










# Multivariate 1
head_length_first_son <- c(191, 195, 181, 176, 208, 189, 188, 192, 179, 183, 190, 188, 163, 186, 181, 
                           192)
head_breadth_first_son <- c(155, 149, 148, 144, 157, 150, 152, 152, 158, 147, 159, 151, 137, 153, 
                            140, 154)
head_length_second_son <- c(179, 201, 185, 171, 192, 190, 197, 186, 187, 174, 195, 187, 161, 173, 
                            182, 185)
head_breadth_second_son <- c(145, 152, 149, 142, 152, 149, 159, 151, 148, 147, 157, 158, 130, 148, 
                             146, 152)
data <- cbind(
  x1 = head_length_first_son,
  x2 = head_breadth_first_son,
  x3 = head_length_second_son,
  x4 = head_breadth_second_son
)

n <- nrow(data)
mean_vector <- apply(data, 2, function(col) sum(col) / n) 
cat("Mean Vector (MLE of μ):\n")
print(mean_vector)
# 2. Covariance Matrix (MLE of Σ)
cov_matrix <- matrix(0, ncol = 4, nrow = 4)
for (i in 1:4) {
  for (j in 1:4) {
    cov_matrix[i, j] <- sum((data[, i] - mean_vector[i]) * (data[, j] - mean_vector[j])) / (n - 1)
  }
}
cat("Covariance Matrix (MLE of Σ):\n")
print(cov_matrix)
# 3. Correlation Matrix (ρ)
cor_matrix <- matrix(0, ncol = 4, nrow = 4)
for (i in 1:4) {
  for (j in 1:4) {
    cor_matrix[i, j] <- cov_matrix[i, j] / sqrt(cov_matrix[i, i] * cov_matrix[j, j])
  }
}
cat("Correlation Matrix (ρ):\n")
print(cor_matrix)
# 4. Conditional Distribution Parameters
S11 <- cov_matrix[1:2, 1:2] # Sub-matrix for (x1, x2)
S12 <- cov_matrix[1:2, 3:4] # Sub-matrix between (x1, x2) and (x3, x4)
S21 <- t(S12) # Transpose of S12
S22 <- cov_matrix[3:4, 3:4] # Sub-matrix for (x3, x4)
# Solving for S11 Inverse using Manual Inversion (2x2 matrix)
inv_S11 <- matrix(0, 2, 2)
det_S11 <- S11[1, 1] * S11[2, 2] - S11[1, 2] * S11[2, 1]
inv_S11[1, 1] <- S11[2, 2] / det_S11
inv_S11[2, 2] <- S11[1, 1] / det_S11
inv_S11[1, 2] <- -S11[1, 2] / det_S11
inv_S11[2, 1] <- -S11[2, 1] / det_S11
# Conditional Covariance: S22.1 = S22 - S21 * inv(S11) * S12
S22_1 <- S22 - S21 %*% inv_S11 %*% S12
cat("Conditional Covariance Matrix (S22.1):\n")
print(S22_1)
# 5. Partial Correlation Between x3 and x4 Given x1, x2
inv_cov <- solve(cov_matrix) # Manually inverted covariance matrix using built-in solve()
r34.12 <- -inv_cov[3, 4] / sqrt(inv_cov[3, 3] * inv_cov[4, 4])
cat("Partial Correlation r34.12:\n", r34.12, "\n")
# 6. Fisher's Z for Confidence Interval
z_value <- 0.5 * log((1 + r34.12) / (1 - r34.12)) # Fisher's Z-transform
se <- 1 / sqrt(n - 4)
lower <- tanh(z_value - 1.96 * se)
upper <- tanh(z_value + 1.96 * se)
cat("95% Confidence Interval for r34.12:\n")
cat("Lower Bound:", lower, "\nUpper Bound:", upper, "\n")
# 7. Sample Multiple Correlation Coefficient
# Manual Calculation for x3 ~ (x1, x2)
X <- as.matrix(cbind(1, data[, 1:2])) # Adding intercept term
Y_x3 <- data[, 3]
beta_x3 <- solve(t(X) %*% X) %*% t(X) %*% Y_x3 # Regression Coefficients
Y_hat_x3 <- X %*% beta_x3
RSS_x3 <- sum((Y_x3 - Y_hat_x3)^2)
TSS_x3 <- sum((Y_x3 - mean(Y_x3))^2)
R_x3 <- sqrt(1 - RSS_x3 / TSS_x3)
cat("Multiple Correlation Coefficient (x3 ~ x1, x2):\n", R_x3, "\n")
#Calculation for x4 ~ (x1, x2)
Y_x4 <- data[, 4]
beta_x4 <- solve(t(X) %*% X) %*% t(X) %*% Y_x4
Y_hat_x4 <- X %*% beta_x4
RSS_x4 <- sum((Y_x4 - Y_hat_x4)^2)
TSS_x4 <- sum((Y_x4 - mean(Y_x4))^2)
R_x4 <- sqrt(1 - RSS_x4 / TSS_x4)
cat("Multiple Correlation Coefficient (x4 ~ x1, x2):\n", R_x4, "\n")













# Multivariate 2
data <- matrix(
  c(15, 14, 19, 18, 
    17, 12, 20, 16, 
    16, 18, 16, 17, 
    16, 16, 15, 15), 
  nrow = 4, byrow = TRUE
)
rownames(data) <- c("Machinist1", "Machinist2", "Machinist3", "Machinist4")
colnames(data) <- c("Machine1", "Machine2", "Machine3", "Machine4")
# Total Observations
N <- length(data) # Total number of observations
n_row <- nrow(data) # Number of Machinists
n_col <- ncol(data) # Number of Machines
# Step 1: Grand Mean
grand_mean <- mean(data)
# Step 2: Total Sum of Squares (SST)
SST <- sum((data - grand_mean)^2)
# Step 3: Row Sum of Squares (SSR - for Machinists)
row_means <- rowMeans(data)
SSR <- n_col * sum((row_means - grand_mean)^2)
# Step 4: Column Sum of Squares (SSC - for Machines)
col_means <- colMeans(data)
SSC <- n_row * sum((col_means - grand_mean)^2)
# Step 5: Interaction Sum of Squares (SSI)
interaction_component <- 0
for (i in 1:n_row) {
  for (j in 1:n_col) {
    interaction_component <- interaction_component + 
      (data[i, j] - row_means[i] - col_means[j] + grand_mean)^2
  }
}
SSI <- interaction_component
# Step 6: Residual/Error Sum of Squares (SSE)
SSE <- SST - SSR - SSC - SSI
# Step 7: Mean Squares
MSR <- SSR / (n_row - 1) # Mean Square for Rows (Machinists)
MSC <- SSC / (n_col - 1) # Mean Square for Columns (Machines)
MSI <- SSI / ((n_row - 1) * (n_col - 1)) # Mean Square for Interaction
MSE <- SSE / ((n_row - 1) * (n_col - 1)) # Mean Square for Error
# Step 8: F-statistics
F_Rows <- MSR / MSE # F-statistic for Rows
F_Columns <- MSC / MSE # F-statistic for Columns
F_Interaction <- MSI / MSE # F-statistic for Interaction
# Step 9: Display Results
cat("Two-Way ANOVA Results:\n")
cat("-------------------------------------------------\n")
cat("SST (Total Sum of Squares):", SST, "\n")
cat("SSR (Machinist Sum of Squares):", SSR, "\n")
cat("SSC (Machine Sum of Squares):", SSC, "\n")
cat("SSI (Interaction Sum of Squares):", SSI, "\n")
cat("SSE (Error Sum of Squares):", SSE, "\n\n")
cat("Mean Squares:\n")
cat("MSR (Rows):", MSR, "\n")
cat("MSC (Columns):", MSC, "\n")
cat("MSI (Interaction):", MSI, "\n")
cat("MSE (Error):", MSE, "\n\n")
cat("F-statistics:\n")
cat("F for Rows (Machinists):", F_Rows, "\n")
cat("F for Columns (Machines):", F_Columns, "\n")
cat("F for Interaction:", F_Interaction, "\n")










# Biostatistics 01
# (a) Relative risk for city A (relaxed gun laws)
shootings_A <- 50
population_A <- 100000
risk_A <- shootings_A / population_A
shootings_B <- 10
population_B <- 100000
risk_B <- shootings_B / population_B
# Relative risk of gun violence in city A
relative_risk_A <- risk_A / risk_B
cat("Relative Risk of gun violence in city A:", relative_risk_A, "\n")
# (b) Relative risk for city B (strict gun laws)
relative_risk_B <- risk_B / risk_A
cat("Relative Risk of gun violence in city B:", relative_risk_B, "\n")
cat("This means the risk of gun violence in city B is 20% of the risk in city A, or 5 times lower.:\n")
# (c) Additional questions to consider:
cat("Questions to consider before concluding association:\n")
cat("1. Are the populations comparable in demographics and economic factors?\n")
cat("2. Could other factors (like law enforcement or economic conditions) influence gun violence 
rates?\n")
cat("3. Are the data collection periods the same for both cities?\n")
cat("4. Is the gun law the only difference between the cities?\n")
cat("5. How is gun violence measured (e.g., homicides, accidents, suicides)?\n")










# Biostatistics 02
cases_no_calcium <- 75
cases_use_calcium <- 25
non_cases_no_calcium <- 25
non_cases_use_calcium <- 75
# (b) Calculate odds of exposure in cases and non-cases
odds_cases <- cases_no_calcium / cases_use_calcium
odds_non_cases <- non_cases_no_calcium / non_cases_use_calcium
cat("Odds of exposure in cases:", odds_cases, "\n")
cat("Odds of exposure in non-cases:", odds_non_cases, "\n")
# (c) Calculate the odds ratio using cross-product
odds_ratio <- (cases_no_calcium * non_cases_use_calcium) / (cases_use_calcium * 
                                                              non_cases_no_calcium)
cat("Odds Ratio:", odds_ratio, "\n")
# (d) Compare prevalence ratio and odds ratio
prevalence_ratio <- (cases_no_calcium / 100) / (non_cases_no_calcium / 100)
cat("Prevalence Ratio:", prevalence_ratio, "\n")
cat("The odds ratio magnifies the difference compared to the prevalence ratio.\n")
#Key Difference: 
cat("The prevalence ratio (3) compares relative exposure percentages, 
while the odds ratio (9) compares the likelihood of exposure between groups. 
The odds ratio magnifies differences, especially for common events, making it a stronger 
measure of association.")










# Biostatistics 03
# Given data are:
non_obese_non_exposed <- 0.03
non_obese_exposed <- non_obese_non_exposed * 1.2
obese_non_exposed <- non_obese_non_exposed * (2 / 3)
obese_exposed <- obese_non_exposed * 1.2
# Create a data frame to display incidence rates
incidence_table <- data.frame(
  Group = c("Non-Obese, Non-Exposed", "Non-Obese, Exposed", "Obese, Non-Exposed", "Obese, 
Exposed"),
  Incidence_Rate = c(non_obese_non_exposed, non_obese_exposed, obese_non_exposed, 
                     obese_exposed)
)
print(incidence_table)
# Calculating Odds and Odds Ratio for Non-Obese group as an example
odds_non_exposed <- non_obese_non_exposed / (1 - non_obese_non_exposed)
odds_exposed <- non_obese_exposed / (1 - non_obese_exposed)
odds_ratio <- odds_exposed / odds_non_exposed
# Displaying Odds Ratio and Relative Risk
cat("Odds Ratio (Non-Obese):", odds_ratio, "\n")
cat("Relative Risk (given): 1.2\n")








# Biostatistics 04
no_folate_neural <- 631 # Neural tube defects (No folate)
folate_neural <- 24 # Neural tube defects (Folate)
no_folate_premature <- 727 # Premature births (No folate)
folate_premature <- 563 # Premature births (Folate)
# Attributable Risk (AR) calculations
AR_neural <- no_folate_neural - folate_neural
AR_premature <- no_folate_premature - folate_premature
cat("Attributable Risk for Neural Tube Defects:", AR_neural, "per 100,000\n")
cat("Attributable Risk for Premature Births:", AR_premature, "per 100,000\n")






#LDA 01
shape_param <- 3 # Shape 
scale_param <- 10 # Scale
n_samples <- 100 # Number of observations
# Weibull-distributed data
data <- scale_param * (-log(runif(n_samples)))^(1 / shape_param)
# Negative log-likelihood function
neg_log_likelihood <- function(alpha, beta) {
  if (alpha <= 0 || beta <= 0) return(Inf) # Ensure parameters are positive
  n <- length(data)
  log_likelihood <-
    n * log(alpha) - n * log(beta) +
    (alpha - 1) * sum(log(data)) -
    sum((data / beta)^alpha)
  return(-log_likelihood) # Negative log-likelihood
}
# Generate 2D grid for shape (alpha) and scale (beta)
alpha_vals <- seq(2, 4, length.out = 50) # Shape parameter grid
beta_vals <- seq(8, 12, length.out = 50) # Scale parameter grid
likelihood_matrix <- matrix(0, nrow = length(alpha_vals), ncol = length(beta_vals))
# Computing log-likelihood for each combination
for (i in 1:length(alpha_vals)) {
  for (j in 1:length(beta_vals)) {
    likelihood_matrix[i, j] <- -neg_log_likelihood(alpha_vals[i], beta_vals[j])
  }}
# 2D likelihood plot
filled.contour(
  x = alpha_vals, y = beta_vals, z = likelihood_matrix,
  xlab = "Shape (alpha)", ylab = "Scale (beta)",
  main = "2D Likelihood Plot of Weibull Model"
)
# MLE Estimation
result <- optim(par = c(2, 5), fn = function(params) neg_log_likelihood(params[1], params[2]), 
                method = "L-BFGS-B", lower = c(1e-5, 1e-5))
mle_shape <- result$par[1]
mle_scale <- result$par[2]
# Mean failure time
mean_failure_time <- mle_scale * gamma(1 + 1 / mle_shape) # Using Weibull formula
sample_mean <- mean(data) # Sample mean
#results
cat("MLE Shape (alpha):", mle_shape, "\n")
cat("MLE Scale (beta):", mle_scale, "\n")
cat("MLE Mean Failure Time:", mean_failure_time, "\n")
cat("Sample Mean Failure Time:", sample_mean, "\n")








# LDA 02
# Given data
lifetimes <- c(22.3, 26.8, 30.3, 31.9, 32.1, 33.3, 33.7, 33.9, 34.7, 36.1, 36.4, 36.5, 
               36.6, 37.1, 37.6, 38.2, 38.5, 38.7, 38.7, 38.9, 38.9, 39.1, 41.1, 41.1, 
               41.4, 42.4, 43.6, 43.8, 44.0, 45.3, 45.8, 50.4, 51.3, 51.4, 51.5)
# Log-transform the data
log_lifetimes <- log(lifetimes)
# Estimate parameters (mu and sigma) using MLE
mu_hat <- mean(log_lifetimes)
sigma_hat <- sd(log_lifetimes)
# Calculate Mean and Median Time to Failure
mean_time_to_failure <- exp(mu_hat + (sigma_hat^2 / 2))
median_time_to_failure <- exp(mu_hat)
# Survival Function
survival_function <- function(t) {
  1 - pnorm((log(t) - mu_hat) / sigma_hat)
}
# Hazard Function
hazard_function <- function(t) {
  dnorm((log(t) - mu_hat) / sigma_hat) / (t * survival_function(t))
}
# Generate a sequence of times for plotting
time_seq <- seq(min(lifetimes), max(lifetimes), length.out = 100)
# Survival and Hazard values
survival_vals <- survival_function(time_seq)
hazard_vals <- hazard_function(time_seq)
# Plot Survival Curve
plot(time_seq, survival_vals, type = "l", col = "blue", lwd = 2,
     xlab = "Time (weeks)", ylab = "Survival Probability",
     main = "Survival Curve")
# Plot Hazard Curve
plot(time_seq, hazard_vals, type = "l", col = "red", lwd = 2,
     xlab = "Time (weeks)", ylab = "Hazard Rate",
     main = "Hazard Curve")
# results
cat("Estimated Parameters:\n")
cat("Mu (mean of log-lifetimes):", mu_hat, "\n")
cat("Sigma (std. dev. of log-lifetimes):", sigma_hat, "\n\n")
cat("Mean Time to Failure (MTTF):", mean_time_to_failure, "weeks\n")
cat("Median Time to Failure:", median_time_to_failure, "weeks\n")










# LDA 03
death_times <- c(7.35, 8.69, 8.80, 9.63, 9.63, 9.89, 9.98, 10.24, 10.36, 10.37, 
                 10.48, 11.33, 11.39, 12.02, 13.12) # Complete data
censored_times <- rep(20, 10) # Right-censored data
# Combine data
all_times <- c(death_times, censored_times)
status <- c(rep(1, length(death_times)), rep(0, length(censored_times))) # 1=death, 0=censored
# Negative log-likelihood function
neg_log_likelihood <- function(params) {
  alpha <- params[1] # Shape parameter
  beta <- params[2] # Scale parameter
  if (alpha <= 0 || beta <= 0) return(Inf) 
  log_f <- log(alpha / beta) + (alpha - 1) * log(death_times / beta) - (death_times / beta)^alpha
  log_S <--(censored_times / beta)^alpha
  total_log_likelihood <- sum(log_f) + sum(log_S)
  return(-total_log_likelihood) # Negative log-likelihood for minimization
}
# Initial guesses for alpha and beta
initial_guess <- c(1, 15)
# MLE using optim
result <- optim(par = initial_guess, fn = neg_log_likelihood, method = "L-BFGS-B", 
                lower = c(1e-5, 1e-5))
mle_alpha <- result$par[1]
mle_beta <- result$par[2]
# Survival function
survival_function <- function(t) {
  exp(-(t / mle_beta)^mle_alpha)
}
# Hazard function
hazard_function <- function(t) {
  (mle_alpha / mle_beta) * (t / mle_beta)^(mle_alpha - 1)
}
# Generate time sequence for plotting
time_seq <- seq(0, 25, length.out = 100)
# Survival and hazard values
survival_vals <- survival_function(time_seq)
hazard_vals <- hazard_function(time_seq)
# Plotting Survival Curve
plot(time_seq, survival_vals, type = "l", col = "blue", lwd = 2,
     xlab = "Time (days)", ylab = "Survival Probability", main = "Survival Curve")
# Plotting Hazard Rate Curve
plot(time_seq, hazard_vals, type = "l", col = "red", lwd = 2,
     xlab = "Time (days)", ylab = "Hazard Rate", main = "Hazard Curve")
# Results
cat("MLE Estimates:\n")
cat("Shape (alpha):", mle_alpha, "\n")
cat("Scale (beta):", mle_beta, "\n")










