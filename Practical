#Time Series 
library(datasets)
data=Nile
plot(data)
require(graphics)
data1<-ts(data,start=c(1871,1), end=c(1970,12), frequency=12)
decomp<-decompose(data1) 
plot(decomp)
Autocorr<-acf(data1,lag.max = 3,plot=FALSE)
Autocorr
Partial_Autocorr<-acf(data1,lag.max = 3,type=c("partial"),plot=FALSE)
Partial_Autocorr
acf(data1, lag.max = 3, main = "Autocorrelation Function")
pacf(data1,lag.max = 3, main = "Partial Autocorrelation Function")







#Machine Learning 1
data <- c(5.65746599, 5.38283914, 2.79892121, 2.85423660, 2.95252721, 5.42626667,
          7.66239113, -0.18001073, 0.65083500, 2.40276530, -0.09929884, 6.32619215,
          5.03650752, 2.07470777, 1.78019174, 6.12891558, 4.05352439, 2.02686971,
          3.50834853, -2.76449768, 4.98428763, 3.01292677, 2.82448038, 3.98110437,
          5.09371862, 5.97961648, 4.56968496, -0.48814532, 5.08736697, 2.41757609)
kde <- density(data)
kde
naive_density <- function(x, data, h) {
  n <- length(data)
  return(sum((abs(x - data) <= h) / (2 * h)) / n)
}
h <- 1.0 
x_range <- seq(min(data) - 1, max(data) + 1, length.out = 100)
naive_estimates <- sapply(x_range, naive_density, data = data, h = h)
plot(kde, main = "Kernel Density Estimate vs Naïve Density Estimator",
     col = "blue", lwd = 2, ylim = c(0, max(kde$y, naive_estimates)), xlab = "X", ylab = "Density")
lines(x_range, naive_estimates, col = "red", lwd = 2)
legend("topright", legend = c("KDE", "Naïve Density"), col = c("blue", "red"), lty = 1, lwd = 2)








#Machine Learning 2
data("birthwt", package = "MASS")
birthwt <- birthwt
birthwt$low <- as.numeric(birthwt$low) 
train_indices <- sample(1:nrow(birthwt), size = 0.7 * nrow(birthwt)) 
train_data <- birthwt[train_indices, ]
test_data <- birthwt[-train_indices, ]
logistic <- function(x) {
  1 / (1 + exp(-x))
}
fit_logistic <- function(X, y, max_iter = 100, tol = 1e-6) {
  n <- nrow(X)
  p <- ncol(X)
  beta <- matrix(0, nrow = p, ncol = 1)
  epsilon <- 1e-8 
  for (i in 1:max_iter) {
    eta <- X %*% beta 
    mu <- logistic(eta) 
    mu <- pmin(pmax(mu, epsilon), 1 - epsilon)
    W <- diag(as.vector(mu * (1 - mu)), n, n) 
    z <- eta + (y - mu) / (mu * (1 - mu))
    Hessian <- t(X) %*% W %*% X
    gradient <- t(X) %*% (y - mu)
    beta_new <- beta + solve(Hessian + diag(epsilon, p)) %*% gradient 
    
    if (any(is.na(beta_new))) stop("Divergence detected in Newton-Raphson method.")
    if (max(abs(beta_new - beta)) < tol) break
    
    beta <- beta_new
  }
  return(beta)
}
X_train <- cbind(1, as.matrix(train_data[, -1])) 
y_train <- train_data$low
beta <- fit_logistic(X_train, y_train)
logistic_model_eq <- paste("log(odds) =", paste(round(beta, 4), c("(Intercept)", colnames(train_data)[-
                                                                                                        1]), collapse = " + "))
cat("Logistic Model Equation:\n", logistic_model_eq, "\n\n")

X_test <- cbind(1, as.matrix(test_data[, -1]))
y_test <- test_data$low
log_odds <- X_test %*% beta
probs <- logistic(log_odds)
predictions <- ifelse(probs > 0.5, 1, 0)

confusion_matrix <- table(Predicted = predictions, Actual = y_test)
cat("Confusion Matrix:\n")
print(confusion_matrix)

true_positive <- confusion_matrix["1", "1"]
true_negative <- confusion_matrix["0", "0"]
false_positive <- confusion_matrix["1", "0"]
false_negative <- confusion_matrix["0", "1"]
sensitivity <- true_positive / (true_positive + false_negative)
specificity <- true_negative / (true_negative + false_positive)
positive_predictive_value <- true_positive / (true_positive + false_positive)
negative_predictive_value <- true_negative / (true_negative + false_negative)

cat("\nModel Performance Metrics:\n")
cat("Sensitivity (True Positive Rate):", round(sensitivity, 4), "\n")
cat("Specificity (True Negative Rate):", round(specificity, 4), "\n")
cat("Positive Predictive Value (PPV):", round(positive_predictive_value, 4), "\n")
cat("Negative Predictive Value (NPV):", round(negative_predictive_value, 4), "\n")







#Machine Learning 3
shape <- 2
scale <- 5
data <- rgamma(100, shape = shape, scale = scale)
bandwidths <- c(0.1, 0.2, 0.5)
plot(NULL, xlim = range(data), ylim = c(0, 0.3), 
     xlab = "Value", ylab = "Density", 
     main = "Kernel Density Estimates with Gaussian Kernel")
colors <- c("red", "blue", "green")
for (i in 1:length(bandwidths)) {
  bw <- bandwidths[i]
  density_estimate <- density(data, kernel = "gaussian", bw = bw)
  lines(density_estimate, col = colors[i], lwd = 2)
}
legend("topright", legend = paste("Bandwidth =", bandwidths), 
       col = colors, lwd = 2, cex = 1)










# Multivariate 1
head_length_first_son <- c(191, 195, 181, 176, 208, 189, 188, 192, 179, 183, 190, 188, 163, 186, 181, 
                           192)
head_breadth_first_son <- c(155, 149, 148, 144, 157, 150, 152, 152, 158, 147, 159, 151, 137, 153, 
                            140, 154)
head_length_second_son <- c(179, 201, 185, 171, 192, 190, 197, 186, 187, 174, 195, 187, 161, 173, 
                            182, 185)
head_breadth_second_son <- c(145, 152, 149, 142, 152, 149, 159, 151, 148, 147, 157, 158, 130, 148, 
                             146, 152)
data <- cbind(
  x1 = head_length_first_son,
  x2 = head_breadth_first_son,
  x3 = head_length_second_son,
  x4 = head_breadth_second_son
)

n <- nrow(data)
mean_vector <- apply(data, 2, function(col) sum(col) / n) 
cat("Mean Vector (MLE of μ):\n")
print(mean_vector)
# 2. Covariance Matrix (MLE of Σ)
cov_matrix <- matrix(0, ncol = 4, nrow = 4)
for (i in 1:4) {
  for (j in 1:4) {
    cov_matrix[i, j] <- sum((data[, i] - mean_vector[i]) * (data[, j] - mean_vector[j])) / (n - 1)
  }
}
cat("Covariance Matrix (MLE of Σ):\n")
print(cov_matrix)
# 3. Correlation Matrix (ρ)
cor_matrix <- matrix(0, ncol = 4, nrow = 4)
for (i in 1:4) {
  for (j in 1:4) {
    cor_matrix[i, j] <- cov_matrix[i, j] / sqrt(cov_matrix[i, i] * cov_matrix[j, j])
  }
}
cat("Correlation Matrix (ρ):\n")
print(cor_matrix)
# 4. Conditional Distribution Parameters
S11 <- cov_matrix[1:2, 1:2] # Sub-matrix for (x1, x2)
S12 <- cov_matrix[1:2, 3:4] # Sub-matrix between (x1, x2) and (x3, x4)
S21 <- t(S12) # Transpose of S12
S22 <- cov_matrix[3:4, 3:4] # Sub-matrix for (x3, x4)
# Solving for S11 Inverse using Manual Inversion (2x2 matrix)
inv_S11 <- matrix(0, 2, 2)
det_S11 <- S11[1, 1] * S11[2, 2] - S11[1, 2] * S11[2, 1]
inv_S11[1, 1] <- S11[2, 2] / det_S11
inv_S11[2, 2] <- S11[1, 1] / det_S11
inv_S11[1, 2] <- -S11[1, 2] / det_S11
inv_S11[2, 1] <- -S11[2, 1] / det_S11
# Conditional Covariance: S22.1 = S22 - S21 * inv(S11) * S12
S22_1 <- S22 - S21 %*% inv_S11 %*% S12
cat("Conditional Covariance Matrix (S22.1):\n")
print(S22_1)
# 5. Partial Correlation Between x3 and x4 Given x1, x2
inv_cov <- solve(cov_matrix) # Manually inverted covariance matrix using built-in solve()
r34.12 <- -inv_cov[3, 4] / sqrt(inv_cov[3, 3] * inv_cov[4, 4])
cat("Partial Correlation r34.12:\n", r34.12, "\n")
# 6. Fisher's Z for Confidence Interval
z_value <- 0.5 * log((1 + r34.12) / (1 - r34.12)) # Fisher's Z-transform
se <- 1 / sqrt(n - 4)
lower <- tanh(z_value - 1.96 * se)
upper <- tanh(z_value + 1.96 * se)
cat("95% Confidence Interval for r34.12:\n")
cat("Lower Bound:", lower, "\nUpper Bound:", upper, "\n")
# 7. Sample Multiple Correlation Coefficient
# Manual Calculation for x3 ~ (x1, x2)
X <- as.matrix(cbind(1, data[, 1:2])) # Adding intercept term
Y_x3 <- data[, 3]
beta_x3 <- solve(t(X) %*% X) %*% t(X) %*% Y_x3 # Regression Coefficients
Y_hat_x3 <- X %*% beta_x3
RSS_x3 <- sum((Y_x3 - Y_hat_x3)^2)
TSS_x3 <- sum((Y_x3 - mean(Y_x3))^2)
R_x3 <- sqrt(1 - RSS_x3 / TSS_x3)
cat("Multiple Correlation Coefficient (x3 ~ x1, x2):\n", R_x3, "\n")
#Calculation for x4 ~ (x1, x2)
Y_x4 <- data[, 4]
beta_x4 <- solve(t(X) %*% X) %*% t(X) %*% Y_x4
Y_hat_x4 <- X %*% beta_x4
RSS_x4 <- sum((Y_x4 - Y_hat_x4)^2)
TSS_x4 <- sum((Y_x4 - mean(Y_x4))^2)
R_x4 <- sqrt(1 - RSS_x4 / TSS_x4)
cat("Multiple Correlation Coefficient (x4 ~ x1, x2):\n", R_x4, "\n")













# Multivariate 2
data <- matrix(
  c(15, 14, 19, 18, 
    17, 12, 20, 16, 
    16, 18, 16, 17, 
    16, 16, 15, 15), 
  nrow = 4, byrow = TRUE
)
rownames(data) <- c("Machinist1", "Machinist2", "Machinist3", "Machinist4")
colnames(data) <- c("Machine1", "Machine2", "Machine3", "Machine4")
# Total Observations
N <- length(data) # Total number of observations
n_row <- nrow(data) # Number of Machinists
n_col <- ncol(data) # Number of Machines
# Step 1: Grand Mean
grand_mean <- mean(data)
# Step 2: Total Sum of Squares (SST)
SST <- sum((data - grand_mean)^2)
# Step 3: Row Sum of Squares (SSR - for Machinists)
row_means <- rowMeans(data)
SSR <- n_col * sum((row_means - grand_mean)^2)
# Step 4: Column Sum of Squares (SSC - for Machines)
col_means <- colMeans(data)
SSC <- n_row * sum((col_means - grand_mean)^2)
# Step 5: Interaction Sum of Squares (SSI)
interaction_component <- 0
for (i in 1:n_row) {
  for (j in 1:n_col) {
    interaction_component <- interaction_component + 
      (data[i, j] - row_means[i] - col_means[j] + grand_mean)^2
  }
}
SSI <- interaction_component
# Step 6: Residual/Error Sum of Squares (SSE)
SSE <- SST - SSR - SSC - SSI
# Step 7: Mean Squares
MSR <- SSR / (n_row - 1) # Mean Square for Rows (Machinists)
MSC <- SSC / (n_col - 1) # Mean Square for Columns (Machines)
MSI <- SSI / ((n_row - 1) * (n_col - 1)) # Mean Square for Interaction
MSE <- SSE / ((n_row - 1) * (n_col - 1)) # Mean Square for Error
# Step 8: F-statistics
F_Rows <- MSR / MSE # F-statistic for Rows
F_Columns <- MSC / MSE # F-statistic for Columns
F_Interaction <- MSI / MSE # F-statistic for Interaction
# Step 9: Display Results
cat("Two-Way ANOVA Results:\n")
cat("-------------------------------------------------\n")
cat("SST (Total Sum of Squares):", SST, "\n")
cat("SSR (Machinist Sum of Squares):", SSR, "\n")
cat("SSC (Machine Sum of Squares):", SSC, "\n")
cat("SSI (Interaction Sum of Squares):", SSI, "\n")
cat("SSE (Error Sum of Squares):", SSE, "\n\n")
cat("Mean Squares:\n")
cat("MSR (Rows):", MSR, "\n")
cat("MSC (Columns):", MSC, "\n")
cat("MSI (Interaction):", MSI, "\n")
cat("MSE (Error):", MSE, "\n\n")
cat("F-statistics:\n")
cat("F for Rows (Machinists):", F_Rows, "\n")
cat("F for Columns (Machines):", F_Columns, "\n")
cat("F for Interaction:", F_Interaction, "\n")










# Biostatistics 01
# (a) Relative risk for city A (relaxed gun laws)
shootings_A <- 50
population_A <- 100000
risk_A <- shootings_A / population_A
shootings_B <- 10
population_B <- 100000
risk_B <- shootings_B / population_B
# Relative risk of gun violence in city A
relative_risk_A <- risk_A / risk_B
cat("Relative Risk of gun violence in city A:", relative_risk_A, "\n")
# (b) Relative risk for city B (strict gun laws)
relative_risk_B <- risk_B / risk_A
cat("Relative Risk of gun violence in city B:", relative_risk_B, "\n")
cat("This means the risk of gun violence in city B is 20% of the risk in city A, or 5 times lower.:\n")
# (c) Additional questions to consider:
cat("Questions to consider before concluding association:\n")
cat("1. Are the populations comparable in demographics and economic factors?\n")
cat("2. Could other factors (like law enforcement or economic conditions) influence gun violence 
rates?\n")
cat("3. Are the data collection periods the same for both cities?\n")
cat("4. Is the gun law the only difference between the cities?\n")
cat("5. How is gun violence measured (e.g., homicides, accidents, suicides)?\n")










# Biostatistics 02
cases_no_calcium <- 75
cases_use_calcium <- 25
non_cases_no_calcium <- 25
non_cases_use_calcium <- 75
# (b) Calculate odds of exposure in cases and non-cases
odds_cases <- cases_no_calcium / cases_use_calcium
odds_non_cases <- non_cases_no_calcium / non_cases_use_calcium
cat("Odds of exposure in cases:", odds_cases, "\n")
cat("Odds of exposure in non-cases:", odds_non_cases, "\n")
# (c) Calculate the odds ratio using cross-product
odds_ratio <- (cases_no_calcium * non_cases_use_calcium) / (cases_use_calcium * 
                                                              non_cases_no_calcium)
cat("Odds Ratio:", odds_ratio, "\n")
# (d) Compare prevalence ratio and odds ratio
prevalence_ratio <- (cases_no_calcium / 100) / (non_cases_no_calcium / 100)
cat("Prevalence Ratio:", prevalence_ratio, "\n")
cat("The odds ratio magnifies the difference compared to the prevalence ratio.\n")
#Key Difference: 
cat("The prevalence ratio (3) compares relative exposure percentages, 
while the odds ratio (9) compares the likelihood of exposure between groups. 
The odds ratio magnifies differences, especially for common events, making it a stronger 
measure of association.")










# Biostatistics 03
# Given data are:
non_obese_non_exposed <- 0.03
non_obese_exposed <- non_obese_non_exposed * 1.2
obese_non_exposed <- non_obese_non_exposed * (2 / 3)
obese_exposed <- obese_non_exposed * 1.2
# Create a data frame to display incidence rates
incidence_table <- data.frame(
  Group = c("Non-Obese, Non-Exposed", "Non-Obese, Exposed", "Obese, Non-Exposed", "Obese, 
Exposed"),
  Incidence_Rate = c(non_obese_non_exposed, non_obese_exposed, obese_non_exposed, 
                     obese_exposed)
)
print(incidence_table)
# Calculating Odds and Odds Ratio for Non-Obese group as an example
odds_non_exposed <- non_obese_non_exposed / (1 - non_obese_non_exposed)
odds_exposed <- non_obese_exposed / (1 - non_obese_exposed)
odds_ratio <- odds_exposed / odds_non_exposed
# Displaying Odds Ratio and Relative Risk
cat("Odds Ratio (Non-Obese):", odds_ratio, "\n")
cat("Relative Risk (given): 1.2\n")








# Biostatistics 04
no_folate_neural <- 631 # Neural tube defects (No folate)
folate_neural <- 24 # Neural tube defects (Folate)
no_folate_premature <- 727 # Premature births (No folate)
folate_premature <- 563 # Premature births (Folate)
# Attributable Risk (AR) calculations
AR_neural <- no_folate_neural - folate_neural
AR_premature <- no_folate_premature - folate_premature
cat("Attributable Risk for Neural Tube Defects:", AR_neural, "per 100,000\n")
cat("Attributable Risk for Premature Births:", AR_premature, "per 100,000\n")






#LDA 01
shape_param <- 3 # Shape 
scale_param <- 10 # Scale
n_samples <- 100 # Number of observations
# Weibull-distributed data
data <- scale_param * (-log(runif(n_samples)))^(1 / shape_param)
# Negative log-likelihood function
neg_log_likelihood <- function(alpha, beta) {
  if (alpha <= 0 || beta <= 0) return(Inf) # Ensure parameters are positive
  n <- length(data)
  log_likelihood <-
    n * log(alpha) - n * log(beta) +
    (alpha - 1) * sum(log(data)) -
    sum((data / beta)^alpha)
  return(-log_likelihood) # Negative log-likelihood
}
# Generate 2D grid for shape (alpha) and scale (beta)
alpha_vals <- seq(2, 4, length.out = 50) # Shape parameter grid
beta_vals <- seq(8, 12, length.out = 50) # Scale parameter grid
likelihood_matrix <- matrix(0, nrow = length(alpha_vals), ncol = length(beta_vals))
# Computing log-likelihood for each combination
for (i in 1:length(alpha_vals)) {
  for (j in 1:length(beta_vals)) {
    likelihood_matrix[i, j] <- -neg_log_likelihood(alpha_vals[i], beta_vals[j])
  }}
# 2D likelihood plot
filled.contour(
  x = alpha_vals, y = beta_vals, z = likelihood_matrix,
  xlab = "Shape (alpha)", ylab = "Scale (beta)",
  main = "2D Likelihood Plot of Weibull Model"
)
# MLE Estimation
result <- optim(par = c(2, 5), fn = function(params) neg_log_likelihood(params[1], params[2]), 
                method = "L-BFGS-B", lower = c(1e-5, 1e-5))
mle_shape <- result$par[1]
mle_scale <- result$par[2]
# Mean failure time
mean_failure_time <- mle_scale * gamma(1 + 1 / mle_shape) # Using Weibull formula
sample_mean <- mean(data) # Sample mean
#results
cat("MLE Shape (alpha):", mle_shape, "\n")
cat("MLE Scale (beta):", mle_scale, "\n")
cat("MLE Mean Failure Time:", mean_failure_time, "\n")
cat("Sample Mean Failure Time:", sample_mean, "\n")








# LDA 02
# Given data
lifetimes <- c(22.3, 26.8, 30.3, 31.9, 32.1, 33.3, 33.7, 33.9, 34.7, 36.1, 36.4, 36.5, 
               36.6, 37.1, 37.6, 38.2, 38.5, 38.7, 38.7, 38.9, 38.9, 39.1, 41.1, 41.1, 
               41.4, 42.4, 43.6, 43.8, 44.0, 45.3, 45.8, 50.4, 51.3, 51.4, 51.5)
# Log-transform the data
log_lifetimes <- log(lifetimes)
# Estimate parameters (mu and sigma) using MLE
mu_hat <- mean(log_lifetimes)
sigma_hat <- sd(log_lifetimes)
# Calculate Mean and Median Time to Failure
mean_time_to_failure <- exp(mu_hat + (sigma_hat^2 / 2))
median_time_to_failure <- exp(mu_hat)
# Survival Function
survival_function <- function(t) {
  1 - pnorm((log(t) - mu_hat) / sigma_hat)
}
# Hazard Function
hazard_function <- function(t) {
  dnorm((log(t) - mu_hat) / sigma_hat) / (t * survival_function(t))
}
# Generate a sequence of times for plotting
time_seq <- seq(min(lifetimes), max(lifetimes), length.out = 100)
# Survival and Hazard values
survival_vals <- survival_function(time_seq)
hazard_vals <- hazard_function(time_seq)
# Plot Survival Curve
plot(time_seq, survival_vals, type = "l", col = "blue", lwd = 2,
     xlab = "Time (weeks)", ylab = "Survival Probability",
     main = "Survival Curve")
# Plot Hazard Curve
plot(time_seq, hazard_vals, type = "l", col = "red", lwd = 2,
     xlab = "Time (weeks)", ylab = "Hazard Rate",
     main = "Hazard Curve")
# results
cat("Estimated Parameters:\n")
cat("Mu (mean of log-lifetimes):", mu_hat, "\n")
cat("Sigma (std. dev. of log-lifetimes):", sigma_hat, "\n\n")
cat("Mean Time to Failure (MTTF):", mean_time_to_failure, "weeks\n")
cat("Median Time to Failure:", median_time_to_failure, "weeks\n")










# LDA 03
death_times <- c(7.35, 8.69, 8.80, 9.63, 9.63, 9.89, 9.98, 10.24, 10.36, 10.37, 
                 10.48, 11.33, 11.39, 12.02, 13.12) # Complete data
censored_times <- rep(20, 10) # Right-censored data
# Combine data
all_times <- c(death_times, censored_times)
status <- c(rep(1, length(death_times)), rep(0, length(censored_times))) # 1=death, 0=censored
# Negative log-likelihood function
neg_log_likelihood <- function(params) {
  alpha <- params[1] # Shape parameter
  beta <- params[2] # Scale parameter
  if (alpha <= 0 || beta <= 0) return(Inf) 
  log_f <- log(alpha / beta) + (alpha - 1) * log(death_times / beta) - (death_times / beta)^alpha
  log_S <--(censored_times / beta)^alpha
  total_log_likelihood <- sum(log_f) + sum(log_S)
  return(-total_log_likelihood) # Negative log-likelihood for minimization
}
# Initial guesses for alpha and beta
initial_guess <- c(1, 15)
# MLE using optim
result <- optim(par = initial_guess, fn = neg_log_likelihood, method = "L-BFGS-B", 
                lower = c(1e-5, 1e-5))
mle_alpha <- result$par[1]
mle_beta <- result$par[2]
# Survival function
survival_function <- function(t) {
  exp(-(t / mle_beta)^mle_alpha)
}
# Hazard function
hazard_function <- function(t) {
  (mle_alpha / mle_beta) * (t / mle_beta)^(mle_alpha - 1)
}
# Generate time sequence for plotting
time_seq <- seq(0, 25, length.out = 100)
# Survival and hazard values
survival_vals <- survival_function(time_seq)
hazard_vals <- hazard_function(time_seq)
# Plotting Survival Curve
plot(time_seq, survival_vals, type = "l", col = "blue", lwd = 2,
     xlab = "Time (days)", ylab = "Survival Probability", main = "Survival Curve")
# Plotting Hazard Rate Curve
plot(time_seq, hazard_vals, type = "l", col = "red", lwd = 2,
     xlab = "Time (days)", ylab = "Hazard Rate", main = "Hazard Curve")
# Results
cat("MLE Estimates:\n")
cat("Shape (alpha):", mle_alpha, "\n")
cat("Scale (beta):", mle_beta, "\n")

















#                       Practical 01
# 1. Generate sufficient random numbers from exponential distribution where the parameter is of your choice. Let consider these generated numbers as a lifetime of the patients then obtain the patient lifetime using the following censoring schemes:

# Right Censoring
# Type-I Censoring Scheme
# Type-II Censoring Scheme
# Random Censoring
# Left Censoring
# Interval Censoring

# Also compare the censored data with the complete data.

set.seed(29)
rate <- 0.1
lifetimes <- rexp(100, rate)
lifetimes
summary(lifetimes)

# Type 1 censoring
ctime <- 15
ctime
cdata <- ifelse(lifetimes<=ctime,lifetimes, ctime)
cstuts <- ifelse(lifetimes<=ctime,1, 0)
type1.data <- cbind(cdata, cstuts)
print(type1.data)


# Type 2 Censoring
ctime = sort(lifetimes)[80] 
ctime
cdata <- ifelse(lifetimes<=ctime,lifetimes, ctime)
cstuts <- ifelse(lifetimes<=ctime,1, 0)
type2.data <- cbind(cdata, cstuts)
print(type2.data)


# Random Censoring
ctime <- rexp(100, rate)
ctime
cdata <- ifelse(lifetimes<=ctime,lifetimes, ctime)
cstuts <- ifelse(lifetimes<=ctime,1, 0)
random.data <- cbind(cdata, cstuts)
print(random.data)

# Left Censoring 
ctime <- 3
ctime
cdata <- ifelse(lifetimes<=ctime, ctime, lifetimes)
cstuts <- ifelse(lifetimes<=ctime, 0,1)
left.data <- cbind(cdata, cstuts)
print(left.data)

# Interval Censoring

intervals <- seq(0,60, 6)
intervals
freq <- table(cut(lifetimes, breaks = intervals))
print(freq)









#############################################################################
#                            Practical 02
# Obtain a sufficiently large sample from an exponential distribution under a Type II
# censoring scheme. After generating the sample, calculate the maximum likelihood
# (ML) estimate of the distribution parameter. Also, evaluate the performance of the
# estimate by computing the bias, variance, and mean squared error (MSE) for different
# sample sizes.



table <- data.frame(
  no_obs = c(),
  no_cen_obs = c(),
  lamda = c(),
  lamda_hat = c(),
  bias = c(),
  variance = c(),
  MSE = c()
)
lamda = 0.1
no_of_cen_obs = c(10,20,30,40,50,60,70,80,90,100)

for( k in no_of_cen_obs){
lamda_tem <- numeric(100)
for(i in 1:100){
  set.seed(i+1)
  lifetimes <- rexp(100, lamda)
  ctime = sort(lifetimes)[k] 
  cdata <- ifelse(lifetimes<=ctime,lifetimes, ctime)
  lamda_tem[i] = k/sum(cdata)
}
var = var(lamda_tem)
mse = sum((lamda_tem- lamda)^2)/100
lamda_hat= mean(lamda_tem)
bias = lamda_hat - lamda
new_row = data.frame(no_obs = 100,
              no_cen_obs = k,
              lamda = lamda,
              lamda_hat = lamda_hat,
              bias = bias,
              variance = var,
              MSE = mse)
table = rbind(table, new_row)

}
table







#####################################################################################
#                               Practical 03
# Obtain a sufficiently large sample from a Weibull distribution under a Type I
# censoring scheme. After generating the sample, calculate the maximum likelihood
# (ML) estimate of the distribution parameters. Also, evaluate the performance of the
# estimate by computing the bias, variance, and mean squared error (MSE) for different
# sample sizes.

# Parameters
set.seed(123)
n_values <- c(100, 200, 300)       # Sample sizes
true_shape <- 2                    # True Weibull shape parameter
true_scale <- 3                    # True Weibull scale parameter
censoring_time <- 5                # Censoring time for Type-I censoring
simulation <- 1000                 # Number of simulations

# Log-likelihood function for censored Weibull data
weibull_log_likelihood <- function(params, data, censoring_time) {
  shape <- params[1]; scale <- params[2]
  uncensored <- data[data <= censoring_time]
  censored <- data[data > censoring_time]
  
  ll_uncensored <- sum(dweibull(uncensored, shape, scale, log = TRUE))
  ll_censored <- sum(pweibull(censored, shape, scale, lower.tail = FALSE, log.p = TRUE))
  
  return(-(ll_uncensored + ll_censored))  
}


evaluate_performance <- function(n, shape, scale, censoring_time, simulation) {
  shape_estimates <- scale_estimates <- numeric(simulation)
  
  for (i in 1:simulation) {
    life <- rweibull(n, shape, scale)
    censored_life <- pmin(life, censoring_time)
    
    fit <- optim(par = c(1, 1), fn = weibull_log_likelihood, data = censored_life,
                 censoring_time = censoring_time, method = "L-BFGS-B", lower = c(0.1, 0.1))
    shape_estimates[i] <- fit$par[1]
    scale_estimates[i] <- fit$par[2]
  }
  

  return(list(
    bias_shape = mean(shape_estimates) - shape, 
    bias_scale = mean(scale_estimates) - scale,
    var_shape = var(shape_estimates), 
    var_scale = var(scale_estimates),
    mse_shape = mean((shape_estimates - shape)^2),
    mse_scale = mean((scale_estimates - scale)^2)
  ))
}

# Run simulations and collect results
results <- data.frame()
for (n in n_values) {
  perf <- evaluate_performance(n, true_shape, true_scale, censoring_time, simulation)
  results <- rbind(results, data.frame(
    n = n, shape_hat = true_shape - perf$bias_shape, scale_hat = true_scale - perf$bias_scale,
    bias_shape = perf$bias_shape, bias_scale = perf$bias_scale, 
    var_shape = perf$var_shape, var_scale = perf$var_scale,
    mse_shape = perf$mse_shape, mse_scale = perf$mse_scale
  ))
}

print(results)








#######################################################################################
#                             Practical 04
# Obtain a sufficiently large sample from a Weibull distribution under type II censoring
# scheme. After generating the sample, calculate the maximum likelihood (ML)
# estimate of the distribution parameters. Also, evaluate the performance of the estimate
# by computing the bias, variance, and mean squared error (MSE) for different sample
# sizes.

# Parameters
set.seed(123)
n_values <- c(100, 200, 300)     # Sample sizes
true_shape <- 2                  # True Weibull shape parameter
true_scale <- 3                  # True Weibull scale parameter
c2 <- 80                         
simulation <- 1000               

# Log-likelihood function for Type-II censored Weibull data
weibull_log_likelihood <- function(params, data, c2) {
  shape <- params[1]; scale <- params[2]
  uncensored <- data[1:c2]                    # First c2 events are observed (uncensored)
  censored_part <- (length(data) - c2) * log(pweibull(data[c2], shape, scale, lower.tail = FALSE))
  ll_uncensored <- sum(dweibull(uncensored, shape, scale, log = TRUE))
  return(-(ll_uncensored + censored_part))    # Negative log-likelihood
}

# Performance Evaluation for Type-II Censoring
evaluate_performance <- function(n, shape, scale, c2, simulation) {
  shape_estimates <- scale_estimates <- numeric(simulation)
  
  for (i in 1:simulation) {
    # Generate and sort Weibull sample
    life <- sort(rweibull(n, shape, scale))
    
    # Estimate parameters using MLE on censored data
    fit <- optim(par = c(1, 1), fn = weibull_log_likelihood, data = life,
                 c2 = c2, method = "L-BFGS-B", lower = c(0.1, 0.1))
    shape_estimates[i] <- fit$par[1]
    scale_estimates[i] <- fit$par[2]
  }
  
  # Calculate bias, variance, and MSE
  return(list(
    bias_shape = mean(shape_estimates) - shape, 
    bias_scale = mean(scale_estimates) - scale,
    var_shape = var(shape_estimates), 
    var_scale = var(scale_estimates),
    mse_shape = mean((shape_estimates - shape)^2),
    mse_scale = mean((scale_estimates - scale)^2)
  ))
}

# Run simulations and collect results for different sample sizes
results <- data.frame()
for (n in n_values) {
  perf <- evaluate_performance(n, true_shape, true_scale, c2, simulation)
  results <- rbind(results, data.frame(
    n = n, shape_hat = true_shape - perf$bias_shape, scale_hat = true_scale - perf$bias_scale,
    bias_shape = perf$bias_shape, bias_scale = perf$bias_scale, 
    var_shape = perf$var_shape, var_scale = perf$var_scale,
    mse_shape = perf$mse_shape, mse_scale = perf$mse_scale
  ))
}

print(results)










#######################################################################################
#                             Practical 05
# The following data represents the readmission times(weeks) for 21 people of
# leukemia patients. Group 1 is the treatment group and group 2 is the placebo group

# Group 1(treatment): 6,6,6,7,10,13,16,22,23,6+,9+,10+,11+,17+,19+,20+,25+,32+,32+,34+,35+

# Group 2(placebo) : 1,1,2,2,3,4,4,5,5,8,8,8,8,11,11,12,12,15,17,22,23

# Note: + denotes censored
# Estimate the survival and hazard curve for both group by the Kaplan-Meier method.

G1 <- c(6, 6, 6, 7, 10, 13, 16, 22, 23, 6, 9, 10, 11, 17, 19, 20, 25, 32, 32, 34, 35)  
G1_event <- c(rep(1,9), rep(0, 12))
G1_data <- data.frame(time = G1, event = G1_event)

G2 <- c(1, 1, 2, 2, 3, 4, 4, 5, 5, 8, 8, 8, 8, 11, 11, 12, 12, 15, 17, 22, 23)
G2_event <- c(rep(1,21))
G2_data <- data.frame(time = G2, event = G2_event)
# Sort the data 
#G1_data = G1_data[order(G1_data$time),]
#G2_data = G2_data[order(G2_data$time),]


km_estimator <- function(data){
  data = data[order(data$time),]
  n <- length(data$time)
  data$r <- 1:n
  for(i in 1:n){
    if(data$event[i] == 1){
      data$prob[i] <- (n-data$r[i])/(n-data$r[i]+1) 
    }
    else{
      data$prob[i] =0
    }
  }
  data1 <- data[data$event==1,]
  data1$sur <- cumprod(data1$prob)
  data1$cum_harz <- -log(data1$sur)
  return(data1)
}

km1 <- km_estimator(G1_data)
km1
km2 <- km_estimator(G2_data)
km2
plot(km1$time, km1$sur, type= 's', xlim = c(0,35), ylim = c(0,1), ylab = 'Probability', 
      xlab = "Time")
lines(km2$time, km2$sur, type= 's', col = "blue")

plot(km1$time, km1$cum_harz, type= 's', xlim = c(0,35), ylim = c(0,1), ylab = 'Probability', 
     xlab = "Time")
lines(km2$time, km2$cum_harz, type= 's', col = "blue")








#######################################################################################
#                             Practical 06
# A clinical trial investigates the effectiveness of a new drug on 10 lung cancer patients.
# The study started in January 2011 and continued till December 2012. The data is
# divided into two groups. Group A (treated with the new drug) and group B (treated with
# the standard treatment). The survival time is given in months. During the trial, some
# people joined. The data looks like below:
data <- data.frame(
  patient_id = 1:10,
  Group = c(rep("A",5),rep("B",5)),
  survival_time = c(5,12,18,20,24,8,10,15,16,22),
  censored = c(1,1,0,0,1,1,0,1,1,1)
)
View(data)
# Calculate and plot the PL estimate of the probability of surviving 2 years or more for
# both groups. Also, comment on the plots.

# b) Suppose that 10 patients joined at the beginning of 24 months; during those months
# 4 patients died and 6 patients survived. Estimate the proportion of patients in the
# population surviving for 24 months or more if the study terminates at 24 months.

### R CODE

# Create the data frame
data <- data.frame(
  patient_id = 1:10,
  Group = c(rep("A", 5), rep("B", 5)),
  survival_time = c(5, 12, 18, 20, 24, 8, 10, 15, 16, 22),
  censored = c(1, 1, 0, 0, 1, 1, 0, 1, 1, 1)
)

# Define a function to calculate Kaplan-Meier survival estimates
kaplan_meier <- function(survival_time, censored) {
  # Create a data frame of unique survival times
  unique_times <- sort(unique(survival_time))
  
  # Initialize survival probabilities
  survival_prob <- data.frame(time = unique_times, probability = 1)
  
  # Calculate the survival probability at each time point
  for (i in 1:length(unique_times)) {
    # Find patients who experienced events (not censored) at this time
    events_at_time <- sum(survival_time == unique_times[i] & censored == 0)
    # Find the number of patients at risk at this time
    at_risk_at_time <- sum(survival_time >= unique_times[i])
    
    if (at_risk_at_time > 0) {
      survival_prob$probability[i] <- survival_prob$probability[i-1] * (1 - events_at_time / at_risk_at_time)
    }
  }
  return(survival_prob)
}

# Kaplan-Meier estimate for group A
data_A <- data[data$Group == "A",]
KM_A <- kaplan_meier(data_A$survival_time, data_A$censored)

# Kaplan-Meier estimate for group B
data_B <- data[data$Group == "B",]
KM_B <- kaplan_meier(data_B$survival_time, data_B$censored)

# Plot Kaplan-Meier survival curves
plot(KM_A$time, KM_A$probability, type = "s", col = "blue", xlab = "Time (months)", ylab = "Survival Probability", main = "Kaplan-Meier Survival Curve")
lines(KM_B$time, KM_B$probability, type = "s", col = "red")
legend("topright", legend = c("Group A", "Group B"), col = c("blue", "red"), lty = 1)

survival_A_24 <- KM_A$probability[which(KM_A$time >= 24)[1]]
survival_B_24 <- KM_B$probability[which(KM_B$time >= 24)[1]]

cat("Survival probability for Group A at 24 months:", survival_A_24, "\n")
cat("Survival probability for Group B at 24 months:", survival_B_24, "\n")

surviving_24_initial <- sum(data$survival_time >= 24 & data$censored == 1) / length(data$survival_time)

new_patients_survived <- 6
new_patients_total <- 10
new_patients_surviving_prop <- new_patients_survived / new_patients_total

overall_surviving_24 <- (sum(data$survival_time >= 24 & data$censored == 1) + new_patients_survived) / (length(data$survival_time) + new_patients_total)

cat("Overall survival proportion for 24 months or more:", overall_surviving_24, "\n")










#######################################################################################
#                             Practical 07
# 1. Generate sufficient random numbers from a normal distribution where the parameters
# are of your choices. Construct a normal probability paper and show that the generated
# sample gives the evidence to belong to the normal distribution. Also, comment on
# your result.

# 2. Generate sufficient random numbers from a distribution as per your roll number. Let's
# consider these generated numbers as the failure time of machines. First construct a
# probability paper for fitting purpose and then verify that the generated data well fits on
# the probability paper. Also, comment on your result.

# S.No. Model Name    Exam Roll No.
# 1.    Exponential   01-15
# 2.    Gamma         16-30
# 3.    Weibull       31-45
# 4.    Lognormal     45-60

### Noraml Distribution
set.seed(29)
n <- 1000
mean <- 10
sd <- 5
sample <- rnorm(n, mean, sd)
print(sample)

q1 <- qnorm(seq(0.01,0.99,0.05))
q2 <- qnorm(seq(0.01,0.99,0.05))
plot(q1,q2, type ='n', xlab = "Sample Quantiles", ylab = 'Theoretical Quantiles')
abline(v= q1)
abline(h= q1)

sq <- sort(sample)
tq <- qnorm(seq(0.01, 0.99, length= n))
points(sq, tq, col =2, pch= 18)
abline(a=0 , b=1, col = 'blue')

### Exponential
set.seed(29)
n <- 100
rate = 0.01
sample <- rexp(n, rate)
print(sample)

q1 <- qexp(seq(0.01,0.99,0.05), rate = 0.01)
q2 <- qexp(seq(0.01,0.99,0.05), rate =  0.01)
plot(q1,q2, type ='n', xlab = "Sample Quantiles", ylab = 'Theoretical Quantiles')
abline(v= q1)
abline(h= q1)
sq <- sort(sample)
tq <- qexp(seq(0.01, 0.99, length= n), rate = 0.01)
points(sq, tq, col =2, pch= 18)
abline(a=0 , b=1, col = 'blue')


###  Gamma 

set.seed(29)
n <- 100
shape = 10
sample <- rgamma(n, shape)
print(sample)

q1 <- qgamma(seq(0.01,0.99,0.05), shape = 10)
q2 <- qgamma(seq(0.01,0.99,0.05), shape =  10)
plot(q1,q2, type ='n', xlab = "Sample Quantiles", ylab = 'Theoretical Quantiles')
abline(v= q1)
abline(h= q1)
sq <- sort(sample)
tq <- qgamma(seq(0.01, 0.99, length= n), shape =10)
points(sq, tq, col =2, pch= 18)
abline(a=0 , b=1, col = 'blue')

### Weibull 

set.seed(29)
n <- 100
shape = 10
scale = 1
sample <- rweibull(n, shape,scale = 1)
print(sample)

q1 <- qweibull(seq(0.01,0.99,0.05), shape = 10)
q2 <- qweibull(seq(0.01,0.99,0.05), shape =  10)
plot(q1,q2, type ='n', xlab = "Sample Quantiles", ylab = 'Theoretical Quantiles')
abline(v= q1)
abline(h= q1)
sq <- sort(sample)
tq <- qweibull(seq(0.01, 0.99, length= n), shape =10)
points(sq, tq, col =2, pch= 18)
abline(a=0 , b=1, col = 'blue')

### Lognormal 








#######################################################################################
#                             Practical 08
data <- data.frame(patient_ID = 1:30,
                   failure_time = c(16.5,11.7,25.3,7.8,19.2,10.6,22.7,5.1,13.9,24.6,17.3,
                                    8.4,28.2,6.7,20.5,15.4,9.9,18.7,30.6,12.3,21.1,4.9,
                                    13.5,16.1,6.2,19.8,27.4,14.7,23.9,26.5))
# Suppose that the above data has been extracted from an experiment. The data represents
# the lifetime of patients. First, make a probability plot for the data and verify the
# distribution from which the data has been generated, and then estimate the parameter
# by using a graphical method.

qqnorm(data$failure_time, main = "Q-Q Plot for Failure Time")
qqline(data$failure_time, col = "red")

qqnorm(data$failure_time, main = "Q-Q Plot against Normal Distribution")
qqline(data$failure_time, col = "red")

mean_failure_time <- mean(data$failure_time)
sd_failure_time <- sd(data$failure_time)

cat("Estimated Mean: ", mean_failure_time, "\n")
cat("Estimated Standard Deviation: ", sd_failure_time, "\n")









#######################################################################################
#                             Practical 09

# The remission times of 42 patients with acute leukemia were reported in a clinical trial
# to assess the ability of 6-mercaptopurine(6-MP) to maintain remission. Patients were
# randomly selected to receive 6-MP or placebo. The study was terminated after one year.
# The following remission times, in weeks, were recorded:

#6-MP (21 patients) : 6,6,6,7,10,13,16,22,23,6+,9+,10+,11+,17+,19+,20+,25+,32+,32+,34+,35+
# Placebo (21 patients) : 1,1,2,2,3,4,4,5,5,8,8,8,8,11,11,12,12,15,17,22,23

# a) Now, fit a distribution to the remission duration of 6-MP patients using the hazard
# plotting technique.
# b) Estimate the parameter/parameters of the distribution.

# Data: Remission times for 6-MP group
remission_times_6MP <- c(6, 6, 6, 7, 10, 13, 16, 22, 23, 6, 9, 10, 11, 17, 19, 20, 25, 32, 32, 34, 35)
status_6MP <- c(rep(1, 9), rep(0, 12))  # 1 = observed, 0 = censored

# Sort remission times in increasing order
sorted_times <- sort(remission_times_6MP)

# Hazard plotting: Create ranks for plotting
ranks <- rank(sorted_times)

# Calculate the cumulative hazard H(t)
n <- length(sorted_times)
cum_hazard <- (ranks - 0.5) / n

# Plot the empirical hazard function (log-log plot)
plot(log(sorted_times), log(-log(1 - cum_hazard)), 
     xlab = "log(Time (weeks))", ylab = "log(-log(1 - F(t)))", 
     main = "Hazard Plot for 6-MP Patients", pch = 16)


plot(sorted_times, -log(1 - cum_hazard), 
     xlab = "log(Time (weeks))", ylab = "log(-log(1 - F(t)))", 
     main = "Hazard Plot for 6-MP Patients", pch = 16)


# Estimate Weibull parameters (Shape and Scale)
# Linear regression on log-log transformed data
X <- log(sorted_times)
Y <- log(-log(1 - cum_hazard))

# Linear regression (manually without using lm() function)
n <- length(X)
x_mean <- mean(X)
y_mean <- mean(Y)

# Calculate slope (beta) and intercept (alpha)
beta <- sum((X - x_mean) * (Y - y_mean)) / sum((X - x_mean)^2)
alpha <- y_mean - beta * x_mean

# Weibull parameters:
# Shape parameter (k) is the slope
shape_weibull <- beta

# Scale parameter (lambda) can be derived from intercept
scale_weibull <- exp(-alpha / shape_weibull)

cat("Estimated Weibull Parameters: \n")
cat("Shape (k):", shape_weibull, "\n")
cat("Scale (lambda):", scale_weibull, "\n")

# Plot the fitted Weibull line
lines(X, alpha + beta * X, col = "red")
legend("bottomright", legend = c("Data", "Weibull Fit"), col = c("black", "red"), lty = 1)









#######################################################################################
#                             Practical 10

# The following dataset is collected from a clinical trial in which researchers are testing
# the effectiveness of a new drug compared to a standard drug in increasing the survival
# time of cancer patients. Use a non-parametric method such as the Cox-Mantel test to
# determine if the new drug prolongs survival significantly compared to the standard
# drug.

# New drug  : 10, 22, 12+ , 15+, 17+, 19+, 23+
# Standard drug: 7,11,14,17,18,18,19


lifetimes <- data.frame(
  time = c(10, 22, 12, 15, 17, 19, 23, 7, 11, 14, 17, 18, 18, 19),
  event = c(1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1),
  group = c(rep("A", 7), rep("B", 7))
)

r2 <- sum(lifetimes$group =="B" & lifetimes$event == 1)

df_cen <- lifetimes[lifetimes$event == 1, ]
time_counts <- data.frame(table(df_cen$time))
time_counts$Var1 <- as.numeric(as.character(time_counts$Var1))  

df <- data.frame(time = c(), m = c(), G1 = c(), G2 = c(), R = c(), A = c())


for (i in 1:nrow(time_counts)) {
  time_point <- time_counts$Var1[i]
  
  G1 <- sum(lifetimes$group == "A" & lifetimes$time >= time_point)
  G2 <- sum(lifetimes$group == "B" & lifetimes$time >= time_point)
  
  R <- G1 + G2
  A <- G2 / R  
  
  df_row <- data.frame(
    time = time_point,
    m = time_counts$Freq[i],
    G1 = G1,
    G2 = G2,
    R = R,
    A = A
  )
  df <- rbind(df, df_row)
}
print(df)

U <- r2 - sum(df$m * df$A)

I <- sum(df$m* (df$R - df$m)*df$A*(1-df$A)/(df$R - 1))

Z <- U/sqrt(I)
cat("Cox-Mantel Test Statistic (Z):", Z)






#######################################################################################
#                             Practical 11

# The following dataset is collected from a clinical trial in which researchers are testing the
# effectiveness of a new drug compared to a standard drug in increasing the survival time of cancer
# patients. Use a non-parametric method such as the Cox-Mantel test to determine if the new drug
# prolongs survival significantly compared to the standard drug.

# Data for the clinical trial
# '+' indicates censored data
nd <- c(10, 22, 12, 15, 17, 19, 23)
nds <- c(1, 1, 0, 0, 0, 0, 0)  # 1: event occurred, 0: censored

sd <- c(7, 11, 14, 17, 18, 18, 19)
sds <- c(1, 1, 1, 1, 1, 1, 1)  # All events occurred

# Combine data into a single data frame
time <- c(nd, sd)
sts <- c(nds, sds)
grp <- c(rep("ND", length(nd)), rep("SD", length(sd)))

# Function to perform the log-rank test without using any package
lrt <- function(time, sts, grp) {
  ut <- sort(unique(time))
  k <- length(ut)
  
  # Initialize variables
  obs <- rep(0, 2)
  exp <- rep(0, 2)
  var <- 0
  
  for (t in ut) {
    # At risk counts
    arn <- sum(time[grp == "ND"] >= t)
    ars <- sum(time[grp == "SD"] >= t)
    art <- arn + ars
    
    # Events at time t
    en <- sum(time[grp == "ND"] == t & sts[grp == "ND"] == 1)
    es <- sum(time[grp == "SD"] == t & sts[grp == "SD"] == 1)
    et <- en + es
    
    # Update observed and expected events
    obs[1] <- obs[1] + en
    obs[2] <- obs[2] + es
    
    if (art > 0) {
      exn <- arn * (et / art)
      exs <- ars * (et / art)
      exp <- c(exp[1] + exn, exp[2] + exs)
      
      # Variance contribution
      var <- var + (arn * ars * et * (art - et)) / (art^2 * (art - 1))
    }
  }
  
  # Calculate test statistic
  chi_sq <- sum((obs - exp)^2) / var
  p_val <- 1 - pchisq(chi_sq, df = 1)
  
  list(chi_sq = chi_sq, p_val = p_val)
}

# Perform the manual log-rank test
lr_res <- lrt(time, sts, grp)

# Output the results
cat("Log-Rank Test Results (Manual):\n")
cat("Chi-Square Statistic:", lr_res$chi_sq, "\n")
cat("P-Value:", lr_res$p_val, "\n")

# Interpretation
if (lr_res$p_val < 0.05) {
  cat("The difference in survival between the new drug and the standard drug is statistically significant.\n")
} else {
  cat("There is no statistically significant difference in survival between the new drug and the standard drug.\n")
}





#######################################################################################
#                             Practical 12

# Estimate the parameters of Cox Proportional hazard model using the Lung dataset inbuilt in survival package. 
# Perform this with and without package.

# Load required library
library(survival)

# Load the Lung dataset
data("lung")

# Explore the dataset
head(lung)

# Cox Proportional Hazards Model using the survival package
cox_model <- coxph(Surv(time, status) ~ age + sex + ph.ecog, data = lung)
summary(cox_model)

# Cox Proportional Hazards Model without using the survival package
# Function to compute Cox model coefficients using Newton-Raphson method
cox_no_package <- function(time, status, covariates, tol = 1e-6, max_iter = 100) {
  n <- length(time)
  X <- as.matrix(covariates)
  beta <- rep(0, ncol(X))
  iter <- 0
  
  while (iter < max_iter) {

    risk <- exp(X %*% beta)
    
    cum_risk <- rev(cumsum(rev(risk)))
    ind_risk <- risk / cum_risk
    
    score <- t(X) %*% (status - ind_risk)
    
    W <- diag(as.vector(ind_risk * (1 - ind_risk)))
    info <- t(X) %*% W %*% X
    
    beta_new <- beta + solve(info) %*% score
    if (sum(abs(beta_new - beta)) < tol) {
      break
    }
    beta <- beta_new
    iter <- iter + 1
  }
  
  list(coefficients = beta, iterations = iter, converged = (iter < max_iter))
}

# Prepare data
covariates <- lung[, c("age", "sex", "ph.ecog")]
time <- lung$time
status <- lung$status == 2  


cox_result <- cox_no_package(time, status, covariates)
cox_result

# Compare results
cat("Cox Model (Package):\n")
print(summary(cox_model))

cat("Cox Model (Custom Implementation):\n")
print(cox_result)






#######################################################################################
#                             Practical 13

# In a competing risks model with two causes of failure, the times to failure
# T1 and T2 follow exponential distributions with cause-specific hazard rates
# λ1 = 0.02 and λ2 = 0.03, respectively. The observed failure time T is given
# by T = min(T1, T2, C), where C is an independent censoring time uniformly
# distributed on [0, 100]. δ is event indicator.
# (a) Simulate n = 1000 observations of (T, δ).
# (b) Using the simulated data, estimate the cause-specific hazards λ1 and
# λ2.
# (c) Compare the estimated hazards with the true values λ1 = 0.02 and
# λ2 = 0.03.


set.seed(123)

n <- 1000 
lambda1 <- 0.02 
lambda2 <- 0.03  
censor_upper <- 100  


T1 <- rexp(n, rate = lambda1) 
T2 <- rexp(n, rate = lambda2)  
C <- runif(n, min = 0, max = censor_upper)  

T <- pmin(T1, T2, C) 
delta <- ifelse(T == T1, 1, ifelse(T == T2, 2, 0))  

data <- data.frame(T = T, delta = delta)

lambda1_hat <- sum(delta == 1) / sum(T)
lambda2_hat <- sum(delta == 2) / sum(T)

cat("True hazard rates:\n")
cat("Lambda1:", lambda1, "\n")
cat("Lambda2:", lambda2, "\n\n")

cat("Estimated hazard rates:\n")
cat("Lambda1_hat:", round(lambda1_hat, 4), "\n")
cat("Lambda2_hat:", round(lambda2_hat, 4), "\n")

hist(T[delta == 0], breaks = 30, col = "gray", xlim = range(T), main = "Histogram of Observed Times by Event Type",
     xlab = "Observed Time", ylab = "Frequency")
hist(T[delta == 1], breaks = 30, col = rgb(0, 0, 1, 0.5), add = TRUE)  # Cause 1
hist(T[delta == 2], breaks = 30, col = rgb(1, 0, 0, 0.5), add = TRUE)  # Cause 2
legend("topright", legend = c("Censored", "Cause 1", "Cause 2"), fill = c("gray", rgb(0, 0, 1, 0.5), rgb(1, 0, 0, 0.5)))





#######################################################################################
#                             Practical ML 

# Generate 100 obs from a gamma dist with shape par = 2 and scale par = 5. 
# obtain kernel density estimate using the Guassian kernel and plot these estimates.


scale <- 5
shape <- 2
set.seed(987)
data <- rgamma(100, shape = shape, scale = scale)

keanel <- function(x) {
  return((1 / sqrt(2 * pi)) * exp(-0.5 * x^2))
}

# Freedman-Diaconis Rule to calculate bandwidth
bd_fd <- function(data) {
  iqr <- IQR(data)
  n <- length(data)
  return(2 * iqr / n^(1/3))  # Bandwidth formula
}

# Kernel Density Estimate function
kde <- function(data, x_vals, bw) {
  n <- length(data)
  kde_vals <- numeric(length(x_vals))
  
  for (i in seq_along(x_vals)) {
    kde_vals[i] <- sum(keanel((x_vals[i] - data) / bw)) / (n * bw)
  }
  return(kde_vals)
}

bw <- bd_fd(data)

x_vals <- seq(min(data), max(data), length.out = 1000)

kde_vals <- kde(data, x_vals, bw)

hist(data, breaks = 10, probability = TRUE,
     main = "Kernel Density Estimate",
     xlab = "Value", ylab = "Density", border = "blue")
lines(x_vals, kde_vals, col = "red", lwd = 2)
legend("topright", legend = c("Hist", "KDE"),
       fill = c(rgb(0.8, 0.8, 0.8, 0.4), "red"), border = "blue")





#######################################################################################
#                             Practical TIME SERIES

# Q1. For the 'Nile' dataset available in R, obtain the results given below by writing a suitable R program.
# Autocorrelation up to order three.
# Partial Autocorrelation.
# Also plot these using suitable diagram.



data("Nile")

acf <- function(x, l) {
  n <- length(x)
  m <- mean(x)
  num <- sum((x[1:(n - l)] - m) * (x[(l + 1):n] - m))
  denom <- sum((x - m)^2)
  return(num / denom)
}
pacf <- function(x, l) {
  r <- sapply(0:l, function(k) acf(x, k))
  R <- toeplitz(r[1:l])
  phi <- solve(R, r[2:(l + 1)])
  return(phi[l])
}

lags <- 1:3
acf_vals <- sapply(lags, function(l) acf(Nile, l))
pacf_vals <- sapply(lags, function(l) pacf(Nile, l))

# Print results
cat("ACF (Lag 1 to 3):\n", acf_vals, "\n\n")
cat("PACF (Lag 1 to 3):\n", pacf_vals, "\n\n")

# Plot results
par(mfrow = c(1, 2))
barplot(acf_vals, names.arg = lags, col = "skyblue", main = "ACF", xlab = "Lag", ylab = "ACF")
barplot(pacf_vals, names.arg = lags, col = "pink", main = "PACF", xlab = "Lag", ylab = "PACF")




#######################################################################################
#                             Practical Multivarite 

# a)	Find the estimates of parameters of conditional distribution of (x3, x4) given (x1, x2) i.e. find S21S11-1 and S22.1=S22-S21S11-1S12
# b)	Find the partial correlation r34.12
# c)	Use Fisher’s  Z to find a confidence interval for 34.12 with confidence 0.95
# d)	Find the sample multiple correlation coefficients between x3 and (x1, x2) and between x4 and (x1, x2)
# e)	Test the hypothesis that x3 is independent of (x1, x2) and x4  is independent of (x1, x2)

x1 <- c(191,195,181,183,176,208,189,197,188,192,179,183,174,190,188,163,195,186,181,175,192,174,176,197,190)
x2 <- c(155,149,148,153,144,157,150,159,152,150,158,147,150,159,151,137,155,153,145,140,154,143,139,167,163)
x3 <- c(179,201,185,188,171,192,190,189,197,187,186,174,185,195,187,161,183,173,182,165,185,178,176,200,187)
x4 <- c(145,152,149,149,142,152,149,152,159,151,148,147,152,157,158,130,158,148,146,137,152,147,143,158,150)


# Creating Matrix
data <- cbind(x1,x2,x3,x4)

# 1. Mean Vector (MLE of μ)
n <- nrow(data)
mean_vector <- apply(data, 2, function(col) sum(col) / n) # Manual mean calculation
cat("Mean Vector (MLE of μ):\n")
print(mean_vector)

# 2. Covariance Matrix (MLE of Σ)
cov_matrix <- matrix(0, ncol = 4, nrow = 4)
for (i in 1:4) {
  for (j in 1:4) {
    cov_matrix[i, j] <- sum((data[, i] - mean_vector[i]) * (data[, j] - mean_vector[j])) / (n - 1)
  }
}
cat("Covariance Matrix (MLE of Σ):\n")
print(cov_matrix)

# 3. Correlation Matrix (ρ)
cor_matrix <- matrix(0, ncol = 4, nrow = 4)
for (i in 1:4) {
  for (j in 1:4) {
    cor_matrix[i, j] <- cov_matrix[i, j] / sqrt(cov_matrix[i, i] * cov_matrix[j, j])
  }
}
cat("Correlation Matrix (ρ):\n")
print(cor_matrix)

# 4. Conditional Distribution Parameters
S11 <- cov_matrix[1:2, 1:2]      # Sub-matrix for (x1, x2)
S12 <- cov_matrix[1:2, 3:4]      # Sub-matrix between (x1, x2) and (x3, x4)
S21 <- t(S12)                    # Transpose of S12
S22 <- cov_matrix[3:4, 3:4]      # Sub-matrix for (x3, x4)
# Solving for S11 Inverse using Manual Inversion (2x2 matrix)
inv_S11 <- matrix(0, 2, 2)
det_S11 <- S11[1, 1] * S11[2, 2] - S11[1, 2] * S11[2, 1]
inv_S11[1, 1] <- S11[2, 2] / det_S11
inv_S11[2, 2] <- S11[1, 1] / det_S11
inv_S11[1, 2] <- -S11[1, 2] / det_S11
inv_S11[2, 1] <- -S11[2, 1] / det_S11

# Conditional Covariance: S22.1 = S22 - S21 * inv(S11) * S12
S22_1 <- S22 - S21 %*% inv_S11 %*% S12
cat("Conditional Covariance Matrix (S22.1):\n")
print(S22_1)

# 5. Partial Correlation Between x3 and x4 Given x1, x2
inv_cov <- solve(cov_matrix) # Manually inverted covariance matrix using built-in solve()
r34.12 <- -inv_cov[3, 4] / sqrt(inv_cov[3, 3] * inv_cov[4, 4])
cat("Partial Correlation r34.12:\n", r34.12, "\n")

# 6. Fisher's Z for Confidence Interval
z_value <- 0.5 * log((1 + r34.12) / (1 - r34.12)) # Fisher's Z-transform
se <- 1 / sqrt(n - 4)
lower <- tanh(z_value - 1.96 * se)
upper <- tanh(z_value + 1.96 * se)
cat("95% Confidence Interval for r34.12:\n")
cat("Lower Bound:", lower, "\nUpper Bound:", upper, "\n")

# 7. Sample Multiple Correlation Coefficient
# Manual Calculation for x3 ~ (x1, x2)
X <- as.matrix(cbind(1, data[, 1:2]))  # Adding intercept term
Y_x3 <- data[, 3]
beta_x3 <- solve(t(X) %*% X) %*% t(X) %*% Y_x3  # Regression Coefficients
Y_hat_x3 <- X %*% beta_x3
RSS_x3 <- sum((Y_x3 - Y_hat_x3)^2)
TSS_x3 <- sum((Y_x3 - mean(Y_x3))^2)
R_x3 <- sqrt(1 - RSS_x3 / TSS_x3)
cat("Multiple Correlation Coefficient (x3 ~ x1, x2):\n", R_x3, "\n")

#Calculation for x4 ~ (x1, x2)
Y_x4 <- data[, 4]
beta_x4 <- solve(t(X) %*% X) %*% t(X) %*% Y_x4
Y_hat_x4 <- X %*% beta_x4
RSS_x4 <- sum((Y_x4 - Y_hat_x4)^2)
TSS_x4 <- sum((Y_x4 - mean(Y_x4))^2)
R_x4 <- sqrt(1 - RSS_x4 / TSS_x4)
cat("Multiple Correlation Coefficient (x4 ~ x1, x2):\n", R_x4, "\n")








